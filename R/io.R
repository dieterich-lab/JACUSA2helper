#' Read JACUSA2 result file
#'
#' \code{read_result()} Reads data that was generated by JACUSA2 and creates a JACUSA2 result object.
#'
#' @param file String that represents the filename of the JACUSA2 output.
#' @param cond_desc Vector of strings that represent names/descriptions for conditions.
#' @param unpack Boolean indicates if info column should be processed.
#' @param progress Boolean indicates if progress should be monitored.
#' @param cores Integer defines how many cores to use.
#'
#' @importFrom magrittr %>%
#'
#' @export
read_result <- function(file, cond_desc = c(), unpack = FALSE, progress = TRUE, cores = 1) {
  # pre process file
  # parse comments ^(#|##) to determine result/method type and number of conditions
  con <- file(file, "r")
  cond_count <- 0
  if (length(cond_desc) > 0) {
    cond_count <- length(cond_desc)
  }
  skip_lines <- 0
  header_names <- NULL
  jacusa_header <- c()
  # possible result/method types: unknown, call-pileup, rt-arrest, lrt-arrest
  type <- .UNKNOWN_METHOD
  while (TRUE) {
    line = readLines(con, n = 1)
    # quit reading: nothing to read or first no header line 
    if (length(line) == 0 || length(grep("^#", line)) == 0) {
      break
    }
    # count header lines to ignore
    skip_lines <- skip_lines + 1

    if (length(grep("^#contig", line)) > 0) {
      # try to guess result/method by header line
      type <- .guess_file_type(line)
      # type will be valid or .guess_file_type throw error

      # parse and store header
      # fix header: #contig -> contig
      header_names <- sub("^#", "", line);
      header_names <- unlist(strsplit(header_names, "\t"))
      
      # guess number of conditions  
      guessed_cond_count <- .guess_cond_count(type, header_names)
      if (cond_count > 0) {
        if (length(guessed_cond_count) != cond_count) {
          stop("Length of description", 
               length(cond_desc), 
               " and conditions(", cond_count, ") don't match.")
        }
      }
      cond_count <- guessed_cond_count
    } else if (length(grep("^##", line)) > 0) {
        jacusa_header <- c(gsub("^##", "", line), jacusa_header)
    }
    
  }
  # finished pre-processing
  close(con)
  
  # check that a header could be parsed
  if (is.null(header_names)) {
    stop("No header line for file: ", file)
  }

  # check that conditions could be guessed
  if (cond_count < 1) {
    stop("Conditions could not be guessed for file: ", file)
  }

  # read data
  data <- data.table::fread(file, 
                            skip = skip_lines, 
                            sep = "\t",
                            header = FALSE, 
                            showProgress = progress)  
  colnames(data) <- header_names
  
  # create result depending on determined method type 
  result <- .create_result(type, cond_count, data, unpack, cores)
  attr(result, .ATTR_TYPE) <- type
  if (length(cond_desc)) {
    attr(result, .ATTR_COND_DESC) <- cond_desc
  }
  attr(result, .ATTR_HEADER) <- jacusa_header
  result <- sticky::sticky(result)
  id <- info <- filter <- ref <- NULL
  result <- result %>%
    dplyr::select(id, dplyr::everything()) %>%
    dplyr::select(-c(info, filter, ref), c(info, filter, ref))
    

    result
}


# create contig:start|start-end:strand
.coord <- function(data) {
  paste0(
    data$contig,
    ":", ifelse(data$end - data$start == 1, data$start, paste0(data$start, "-", data$end)),
    ":", data$strand
  )
}

# FIXME remove |read_sub
.id <- function(data) {
  coord <- .coord(data)
  regex <- paste0("(", .SUB_TAG_COL, "|read_sub)=([^;]+)")
  if (any(stringr::str_detect(data[[.INFO_COL]], regex))) {
    sub_tag <- stringr::str_match(data[[.INFO_COL]], regex)[, 3]
    sub_tag <- clean_read_sub(sub_tag)
    return(paste0(coord, ":", sub_tag))
  }
  
  coord
}


# Create result for type and conditions from data 
.create_result <- function(type, cond_count, data, unpack, cores) {
  # unique id for rows, important for expanding
  data$id <- .id(data)
  
  cols <- NULL
  if(type == .CALL_PILEUP) {
    cols <- c(.CALL_PILEUP_COL)
  } else if (type %in% c(.RT_ARREST, .LRT_ARREST)) {
    cols <- c(.ARREST_COL, .THROUGH_COL)
  } else {
    stop("Unknown type: ", type)
  }
  result <- .unpack_cols(data, cols, cond_count, .BASES, cores)
  # process arrest and through columns
  if (type %in% c(.RT_ARREST, .LRT_ARREST)) {
    result[[.CALL_PILEUP_COL]] <- mapply_repl(
      Reduce, 
      result[[.ARREST_COL]], result[[.THROUGH_COL]], 
      MoreArgs=list(f="+"), cores = cores
    )
    result <- add_arrest_rate(result)
  }

  # add coverage
  result[[.COV]] <- coverage(result[[.CALL_PILEUP_COL]])

  # unpack info field
  if (unpack) {
    result <- .unpack_info(result, cond_count, cores)
  }

  result
}

#' Observed base calls.
#' 
#' Observed base calls.
#' 
#' @param bases tibble of base call counts
#' @return vector of observed base calls
#' @export
base_call <-function(bases) {
  apply(bases > 0, 1, function(x) {
    paste0(names(which(x)), collapse= "") 
    }
  )
}

# match columns from df  that are prefixed with types
.matches <- function(df, prefixes, cond_count) {
  # regex parts:
  prefix_regex = paste0("^(", paste0(prefixes, collapse = "|"), ")")
  cond_regex = paste0("([:digit:]{", nchar(cond_count), "})")
  repl_regex = "([:digit:]+)"
  # get relevant cols and disect to m(atch):
  # match, base_type, condition, replicate
  cols <- stringr::str_subset(names(df), paste0(prefix_regex, cond_regex, repl_regex))
  m <- stringr::str_match(
    cols,
    paste0(prefix_regex, cond_regex, repl_regex)
  )
  colnames(m) <- c("col", "prefix", "cond", "repl")
  matches <- as.data.frame(m, stringsAsFactors = FALSE)

  matches
}

# replace empty with actual values
.fill_empty <- function(df, cols, new_cols) {
  new_value <- paste0(rep(0, length(new_cols)), collapse = ",")
  df[cols][df[cols] == .EMPTY] <- new_value
  
  df
}

.unpack_info <- function(result, cond_count, cores) {
  info <- tidyr::separate_rows(result[, c("id", .INFO_COL)], info, sep = ";")

  . <- key <- value <- NULL
  x <- strsplit(info[[.INFO_COL]], "=") %>% do.call(rbind, .) %>% as.data.frame() %>% tidyr::as_tibble()
  colnames(x) <- c("key", "value")
  
  info <- info[, "id"]
  info <- dplyr::bind_cols(info, x) %>% tidyr::pivot_wider(names_from = key, values_from=value)
  
  result <- dplyr::inner_join(result, info, by = "id")
  
  # parse indels on demand
  if (any(c("insertion_score", "deletion_score") %in% names(result))) {
    result <- .unpack_indels(result, cond_count, cores)
  }

  result
}

# expand strings from vector s
.unpack <- function(s, new_cols) {
  . <- NULL
  df <- lapply(strsplit(s, ","), as.numeric) %>% 
    do.call(rbind, .) %>% 
    as.data.frame() %>%
    tidyr::as_tibble()
  colnames(df) <- new_cols
  
  df
}

# expand cols
.unpack_cols <- function(df, prefixes, cond_count, new_cols, cores) {
  matches = .matches(df, prefixes, cond_count)
  
  cols <- unique(matches$col)
  df <- .fill_empty(df, cols, new_cols)
  
  . <- NULL
  unpacked <- parallel::mclapply(
    df[cols], 
    .unpack, new_cols=new_cols, 
    mc.cores = min(cols, cores), mc.preschedule = FALSE
  )

  df <- dplyr::select(df, !cols) %>% tidyr::as_tibble()

  for (prefix in prefixes) {
    i <- matches$prefix == prefix
    merged <- .merge_cond(unpacked[matches[i, "col"]], matches[i, ])
    df[[prefix]] <- tidyr::as_tibble(merged)
  }

  df
}

#
.merge_cond <- function(df, matches) {
  conds <- paste0("cond", matches$cond)
  unique_conds <- unique(conds)
  merged <- tapply(df, conds, tidyr::as_tibble, simplify = FALSE)
  
  for (cond in unique_conds) {
    cond_data <- merged[[cond]]
    i <- match(names(cond_data), matches$col)
    names(cond_data) <- paste0("rep", matches[i, "repl"])
    merged[[cond]] <- cond_data
  }
  names(merged) <- unique_conds
  merged <- c(merged)

  merged
}


# helper function to extract deletion info(s)
.unpack_indels <- function(df, cond_count, cores) {
  result <- .unpack_cols(df, c("ins", "del"), cond_count, c("reads", "coverage"), cores)
  meta_cols <- c("deletion_score", "deletion_pvalue", "insertion_score", "insertion_pvalue")
  
  
  for (col in names(result)[names(result) %in% meta_cols]) {
    result[[col]] <- as.numeric(result[[col]])
  }

  result
}
  
#' Read multiple related JACUSA2 results
#' 
#' Read multiple related JACUSA2 results.
#' 
#' @param files Vector of files.
#' @param meta_conds Vector of string vectors that explain files.
#' @param cond_descs Vector of strings that represent names/descriptions for conditions.
#' @param unpack Boolean indicates if info column should be processed.
#' @param cores Integer defines how many cores to use.
#'
#' @export
read_results <- function(files, meta_conds, cond_descs, unpack = FALSE, cores = 1) {
  stopifnot(length(files) == length(meta_conds))
  
  # read all files  
  l <- parallel::mcmapply(function(file, meta_cond, cond_desc) {
    result <- read_result(file, cond_desc, unpack)
    
    type <- attr(result, .ATTR_TYPE)
    attr(result, .ATTR_TYPE) <- NULL
    attr(result, .ATTR_COND_DESC) <- NULL

    result[[.META_COND_COL]] <- meta_cond
    return(list(result, type))
  }, files, meta_conds, cond_descs, mc.cores = min(length(files), cores), SIMPLIFY = FALSE)

  types <- lapply(l, "[[", 2) %>% unlist(use.names = FALSE)

  
  # combine read files
  results <- do.call(rbind, lapply(l, "[[", 1))
  results$meta_cond <- as.factor(results$meta_cond)

  attr(results, .ATTR_TYPE) <- types
  attr(results, .ATTR_COND_DESC) <- cond_descs

  results
}
