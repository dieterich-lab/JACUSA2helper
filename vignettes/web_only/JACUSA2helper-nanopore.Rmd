---
title: "Analysis of Nanopore HEK293 with JACUSA2helper"
author: "Christoph Dieterich, Michael Piechotta"
date: "`r Sys.Date()`"
output: rmarkdown::html_vignette
bibliography: JACUSA2helper-nanopore.bib
link-citations: yes
vignette: >
  %\VignetteIndexEntry{Analysis of Nanopore HEK293 with JACUSA2helper}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
options(tibble.print_min = 4L, tibble.print_max = 4L)
library(JACUSA2helper)
library(GenomicRanges)
library(BSgenome)
library(plyranges)
library(magrittr)
library(dplyr)
library(ggplot2)
library(VennDiagram)


miclip <- read.delim(
  "/storage/JACUSA2_TestField/Nanopore/miCLIP_union_flat_exclude_Y_chromosome.bed", 
  header = FALSE, as.is = TRUE, 
  col.names = c("contig", "start", "end", "overlap", "score", "strand")
)
miclip$start <- miclip$start - 2
miclip$end <- miclip$end + 2
miclip$region <- paste0(miclip$contig, ":", miclip$start + 1, "-", miclip$end, ":", miclip$strand)

set.seed(777)
```

In the following, the workflow for use case 3 from @Piechotta20221 is presented.
Data for [use cases 1-3](https://zenodo.org/record/5930729#.YfmTDFvMIUG) can be downloaded to repeat the analysis.

# General workflow

Use `read_result` to read JACUSA2 output and filter the data set.

To repeat the Nanopore analysis from @Piechotta2021, download the following data:
* [WT100 vs. WT0](https://zenodo.org/record/5940218/files/WT100_vs_WT0_call2_result.out.gz?download=1).
* [WT vs. KO](https://zenodo.org/record/5940218/files/WT_vs_KO_call2_result.out.gz?download=1) and

Depending on your bandwidth the Download might take some time.
Make sure, you have approx. 20GB of main memory available.

## Pre-processing

At the beginning we organize the data:
```{r}
# files to read
files <- c(
  "/storage/zenodo/use_case3/WT_vs_KO_call2_result.out.gz",
  "/storage/zenodo/use_case3/WT100_vs_WT0_call2_result.out.gz"
)

# descriptions corresponding to files 
meta_conds = c(
  "WT_vs_KO",
  "WT100_vs_WT0"
)
```

`meta_conds` is used as a concise description of each file to distinguish the JACUSA2 outputs.

The pre-processing workflow for one file can be summarized into the following steps:

Read
: Read JACUSA2 output.

Filter
: Employ coverage and other filters to remove not interesting sites.

Add meta condition
: Add concise description of each file.

Unpack and add info
: Unpack specific info data and add.

Reduce
: Select relevant data column.

We explain the pre-processing workflow for one file and generalize the to arbitrary number of files - here two.

<!-- TODO mention `read_results` 
### `read_results`

An other option to read the data set is via `read_results` - see `vignette(Introduction to meta conditions with JACUSA2helper)`.
Does not apply here because the data has different number of replicates.
This approach will consume much more main memory. We recommend to read big data sets serially with `read_result` and combine them afterwards.
-->

### Read 
The underlying function of `read_result` is `data.table::fread` - check the respective help page for details on  further options.
Depending on your machine increase the `nThreads` to increase the number of threads to parse the file.

```{r}
i <- 1 # WT_vs_KO
print(files[i])
print(meta_conds[i])

wt_vs_ko_res <- read_result(files[i], nThread = 1)
```

### Filter
We use the following filters:
* retain sites with "A" in the reference,
* retain sites on chromosome 1-22, and X, and
* remove sites that are within homopolymers (JACUSA2 filter flag: "Y").

```{r}
print(paste0("Sites BEFORE filtering: ", length(wt_vs_ko_res)))

wt_vs_ko_filtered <- wt_vs_ko_res %>% filter(
    ref == "A",
    seqnames %in% c(as.character(1:22), "X"),
    ! grepl("Y", filter, fixed = TRUE)
)

print(paste0("Sites AFTER filtering: ", length(wt_vs_ko_filtered)))
```

### Add meta condition

We add a concise description of the file to each result object. This enables to distinguish concatenated results from multiple files later on in the analysis.

```{r}
wt_vs_ko_filtered$meta_cond <- factor(meta_conds[1], meta_conds)

table(wt_vs_ko_filtered$meta_cond)
```

`WT100_vs_WT0` is zero because we haven't added the respective file yet.

### Add specific info

The "info" field contains meta information for sites, such as detailed INDEL statistics.
To save memory, we manually unpack the "info" field and select the following keys:
* "insertion_score" and 
* "deletion_score".
We will use these data during the model training.

```{r}
unpacked_info <- unpack_info(
    wt_vs_ko_filtered$info, 
    cond_count = 2, 
    keys=c("insertion_score", "deletion_score")
  )

  # append specific info
  mcols(wt_vs_ko_filtered) <- cbind(mcols(wt_vs_ko_filtered), unpacked_info)
```

### Reduce

We continue to reduce the data set by selecting only relevant columns and renaming "score" column to "call2_score" for consistency: 

```{r}
print(
  paste0(
    "Dimensions BEFORE reduction: ", 
    paste0(dim(mcols(wt_vs_ko_filtered)), collapse = ", ")
  )
)

mcols(wt_vs_ko_filtered) <- mcols(wt_vs_ko_filtered)[c("meta_cond", "score", "insertion_score", "deletion_score")]
colnames(mcols(wt_vs_ko_filtered))[colnames(mcols(wt_vs_ko_filtered)) == "score"] <- "call2_score"

print(
  paste0(
    "Dimensions AFTER reduction: ", 
    paste0(dim(mcols(wt_vs_ko_filtered)), collapse = ", ")
  )
)
```

This operation concludes the pre-processing workflow for one file.

### Generalize

So far we have shown a detailed description of pre-processing one file. 
In the following, we extend provide code for arbitrary number of results and meta conditions
for pre-processing.

```{r}
# pre-processing workflow for arbitrary number of files and meta conditions
results <- mapply(function(file, meta_cond) {
    result <- read_result(file, nThread = 4) %>%
      filter(
        ref == "A",
        ! grepl("Y", filter, fixed = TRUE),
        seqnames %in% c(as.character(1:22), "X")
      )
    # add file specific condition
    result$meta_cond <- factor(meta_cond, meta_conds)

    # unpack specific info
    unpacked_info <- unpack_info(
      result$info, 
      cond_count = 2, 
      keys=c("insertion_score", "deletion_score")
    )
    # add specific info
    mcols(result) <- cbind(mcols(result), unpacked_info)
    
    # select relevant data 
    mcols(result) <- mcols(result)[c("meta_cond", "score", "insertion_score", "deletion_score")]
    colnames(mcols(result))[colnames(mcols(result)) == "score"] <- "call2_score"

    return(result)
  }, files, meta_conds, SIMPLIFY = FALSE, USE.NAMES = FALSE)

# convert and concatenate GenomicRanges
results <- unlist(GenomicRanges::GRangesList(results))

# filtered sites per file / meta condition
table(results$meta_cond)
```

`results` now consists of locations and scores for sites from all `files` and `meta_conds`.

```{r}
results$id <- JACUSA2helper::id(results)

plt <- venn.diagram(
  tapply(results$id, results$meta_cond, c),
  filename = NULL,
  lwd = 1,
  cex = 0.5,
  fontfamily = "sans",
  cat.cex = 0.3,
  cat.default.pos = "outer",
  cat.fontfamily = "sans",
)
grid.newpage()
grid.draw(plt)
```

## Calculating overlapping regions

We extend the coordinates of a site by 2nt in each direction:
```{r}
extended_sites <- extend(results, left=2, right=2)

unique_regions <- unique(extended_sites)
```

`unique_regions` consists now of 5nt wide coordinates around all sites with the site positioned in the middle (= position 3).
We calculate the position of each site in overlapping `unique_regions`:
```{r}
# compute overlap
hits <- findOverlaps(results, unique_regions)
# corresponding overlap in ...
overlap_regions <- unique_regions[subjectHits(hits)]
# and ...
overlap_results <- results[queryHits(hits)]

# coordinates of overlapping region: contig:start-end:strand
# bin sites in unique_regions
overlap_results$region <- paste0(
  seqnames(overlap_regions), ":",
  start(overlap_regions), "-", end(overlap_regions), ":",
  strand(overlap_regions)
)

# add site position within extended region
overlap_results$position <- start(overlap_results) - start(overlap_regions) + 1

barplot(table(overlap_results$position), xlab = "position of site within a motif\n(original site position is 3)")
```

## Create feature matrix

The pre-processing is almost done, the data frame should contain the following columns:

```{r}
mcols(overlap_results)
```

Use `tidyr::pivot_wider` to create the feature matrix:

```{r}
feature_matrix <- mcols(overlap_results) %>% 
  as.data.frame() %>%
  tidyr::pivot_wider(
    id_cols = region, 
    names_from = c(meta_cond, position), 
    values_from = c(call2_score, insertion_score, deletion_score), 
    names_glue = "{meta_cond}_{.value}_{position}",
    names_sort = TRUE,
    values_fill = 0.0,
  )
```

### Add sequence

In order to add information if a site is contained in a DRACH motif, we need to retrieve sequence information for
the overlapping `unique_regions` of a site.

If you have a custom FASTA sequence, use `Rsamtools::FaFile` to load the FASTA file.
Otherwise, load a genome from via [BSgenome](https://bioconductor.org/packages/release/bioc/html/BSgenome.html).

```{r}
library("BSgenome.Hsapiens.NCBI.GRCh38")

# retrieve sequence and convert to character vector
feature_matrix$motif <- getSeq(BSgenome.Hsapiens.NCBI.GRCh38, GRanges(feature_matrix$region)) %>% 
  as.character() %>% unname()

# number of top 10 motifs
sort(table(feature_matrix$motif), decreasing = TRUE)[1:10]
```

Next, we add an indicator variable if the DRACH motif ([AGT][AG]AC[ACT]) is present:
```{r}
feature_matrix$DRACH <- 0
feature_matrix$DRACH[grep("[AGT][AG]AC[ACT]", feature_matrix$motif)] <- 1

tbl <- table(feature_matrix$DRACH)
names(tbl) <- recode(names(tbl), "0" = "no", "1" = "yes")  %>% paste0(" (", tbl, ")")
pie(tbl, main="DRACH motif present")
```

Use `saveRDS(feature_matrix, file = "feature_matrix.rds")` to store the current state.
Use `feature_matrix <- readRDS("feature_matrix.rds")` to restore previous state.

# Estimation

## Learn from miCLIP Data

Next, we need miCLIP data to train our model:
* Boulias,
* Koertel, and
* Koh.
We use sites that are present in all three data sets.

The column "region" corresponds to the extended sites around identified miCLIP sites.
Make sure that "region" is 1-indexed if you use your own annotation.

```{r}
head(miclip)
```

Intersect regions of feature matrix with miCLIP data (contained in all three data sets: Boulias,Koertel, and Koh):

```{r}
# dimensions of feature matrix BEFORE intersect with miCLIP
print("Dimensions of feature matrix")
dim(feature_matrix)

# index vector of regions contained in miCLIP data
i <- feature_matrix$region %in% miclip[miclip$overlap == "Boulias,Koertel,Koh", "region"]
# retain sites that overlap with miCLIP and remove non-numeric columns
nmf_matrix <- feature_matrix[i, ] %>% 
  as.data.frame()

# dimensions of feature matrix AFTER intersect with miCLIP
print("Dimensions of nmf_matrix after processing")
dim(nmf_matrix)
```

The learning algorithm requires the matrix to consist only of numeric value.
Remove non-numeric columns:
```{r}
# dimensions of feature matrix BEFORE intersect with miCLIP
print("Dimensions of feature matrix BEFORE processing")
dim(nmf_matrix)

rownames(nmf_matrix) <- nmf_matrix$region

# remove non-numeric columns
nmf_matrix <- nmf_matrix %>% select(-region, -motif)

# set sensible defaults
nmf_matrix[is.na(nmf_matrix)] <- 0
nmf_matrix[nmf_matrix < 0] <- 0

# dimensions of feature matrix AFTER intersect with miCLIP
print("Dimensions of feature matrix AFTER processing")
dim(nmf_matrix)
```

## Estimate parameters

Make sure to remove columns such as: "motif" and "region" before you start the learning algorithm.


```{r}
library(NMF)
nmfSeed('nndsvd')
meth <- nmfAlgorithm(version='R')
meth <- c(names(meth), meth)

small_nmf_matrix <- nmf_matrix[, grep("WT_vs_KO", colnames(nmf_matrix))]
# filter zero value rows
i <- rowSums(small_nmf_matrix) == 0
small_nmf_matrix <- small_nmf_matrix[!i, ]

estim.r <- nmf(small_nmf_matrix, 2:10, nrun = 10, seed = 123456, .opt = 'vp3')
V.random <- randomize(small_nmf_matrix)
```

Estimate quality measures from the shuffled data (use default NMF algorithm):
```{r}
estim.r.random <- nmf(V.random, 2:10, nrun=10, seed=123456, .opt='vp3')
```

### Visualize estimation

```{r}
plot(estim.r, estim.r.random)
```

### Choose factorization rank

```{r}
DeltaSil <- estim.r$measures$silhouette.consensus - estim.r.random$measures$silhouette.consensus
DeltaCoph <- estim.r$measures$cophenetic - estim.r.random$measures$cophenetic

#how can we select factorization rank ?
ChoseRank <- min(
  which(DeltaSil == max(DeltaSil)) + 1,
  which(DeltaCoph == max(DeltaCoph)) + 1
)
```

```{r}
val_matrix <- matrix(0, nrow = length(DeltaCoph), ncol = 2)
val_matrix[,1] <- DeltaSil 
val_matrix[,2] <- DeltaCoph

barplot(t(val_matrix), names.arg = 2:(length(DeltaCoph)+1), beside = TRUE, col = c("#009E73", "#D55E00"), legend.text = c("Silhouette", "Cophenetic"),ylim=range(pretty(c(0, DeltaSil))), xlab="Rank", 
        ylab="Delta", main = "Difference between original and randomized data",cex.names=2,cex.axis=2)
abline(h=c(max(DeltaSil) , max(DeltaCoph)) , col=c("#009E73", "#D55E00"))
```

```{r}
final_nmf_matrix <- nmf(small_nmf_matrix, ChoseRank, nrun=10, seed=123456, .opt='vp3')
```

Use `saveRDS(final_nmf_matrix, file="final_nmf_matrix.rds")` to store final result.

## Visualize NMF

```{r}
# basis components
basismap(final_nmf_matrix)
```

#```{r}
mixture coefficients
coefmap(final_nmf_matrix)
#```

```{r}
w <- basis(final_nmf_matrix)
# Table of number of best hits for each pattern"
pattern_instances <- table(apply(w, 1, function(x){ which( x==max(x) ) } ))

barplot(
  height = pattern_instances,
  names = 1:(dim(w)[2]),
  main = "NMF_Patterns_Scoring", 
  xlab = "Patterns", 
  ylab = "Membership Score (basis matrix)",
  ylim = range(pretty(c(0, pattern_instances)))
)
```

<!--
## Visualize patterns

#```{r}
color_palette <- c("#000000", "#E69F00", "#56B4E9", "#009E73", "#F0E442", "#0072B2", "#D55E00", "#CC79A7")
#```

#```{r}
w <- basis(final_nmf_matrix)
h <- coef(final_nmf_matrix)
h <- rbind(h, t(colSums(h)))
#```

#```{r}
## add the combined patterns
#for (pattern in list(c(1,2,4,6), c(1,2,3,4,6))) {
#  h <- rbind(h, t(colSums(h[pattern, ])))
#}
#```

#```{r}
# barplots of patterns
for (k in 1:nrow(h)){
  rep1 <- matrix(h[k, ], ncol = 5, byrow = TRUE)[1:3, ]
  rownames(rep1) <- c("Mismatch","Deletion","Insertion")
  colnames(rep1) <- c("Pos1","Pos2","Pos3","Pos4","Pos5")
  
  barplot(
    rep1,
    main = paste0("NMF_Pattern_", k),
    col = color_palette,
    cex.names = 2,
    cex.axis = 2,
    legend = TRUE
  )
}
```
-->

# References

